# Kanso.AI â€” AI-Powered Project Planning with Multi-Agent System

<div align="center">

**Turn any goal into a detailed, dependency-aware Gantt chart using a collaborative AI agent pipeline**

[![Python 3.11+](https://img.shields.io/badge/Python-3.11+-3776AB.svg?logo=python&logoColor=white)](https://python.org)
[![React 19](https://img.shields.io/badge/React-19-61DAFB.svg?logo=react&logoColor=black)](https://react.dev)
[![Google ADK](https://img.shields.io/badge/Google_ADK-Agent_Framework-4285F4.svg?logo=google&logoColor=white)](https://google.github.io/adk-docs/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-009688.svg?logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)
[![License](https://img.shields.io/badge/License-Apache_2.0-green.svg)](LICENSE)

</div>

---

## Table of Contents

- [What is Kanso.AI?](#-what-is-kansoai)
- [Key Features](#-key-features)
- [How It Works â€” The Agent Pipeline](#-how-it-works--the-agent-pipeline)
- [Architecture Overview](#-architecture-overview)
- [Project Structure](#-project-structure)
- [Getting Started](#-getting-started)
- [Configuration Reference](#-configuration-reference)
- [API Reference](#-api-reference)
- [Data Models](#-data-models)
- [Observability & Evaluation with Opik](#-observability--evaluation-with-opik)
  - [1. Full-Pipeline Tracing](#1-full-pipeline-tracing-via-opiktracer)
  - [2. Rich Trace Metadata](#2-rich-trace-metadata)
  - [3. Datasets & Experiments](#3-datasets--experiments)
  - [4. Custom & Built-in Metrics (9 Total)](#4-custom--built-in-evaluation-metrics-9-total)
  - [5. Online Evaluation Rules](#5-online-evaluation-rules)
  - [6. Agent Optimizer](#6-opik-agent-optimizer)
  - [7. Experiment Results](#7-experiment-results)
- [Technology Stack](#-technology-stack)
- [Development Guide](#-development-guide)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸ¯ What is Kanso.AI?

Kanso.AI is a full-stack application that transforms any goal â€” a startup launch, a wedding, a home renovation, a software project â€” into a **detailed, dependency-aware project plan** visualized as an interactive Gantt chart.

It uses a **multi-agent AI system** built on [Google's Agent Development Kit (ADK)](https://google.github.io/adk-docs/) where specialized agents collaborate through a structured pipeline:

1. **Analyze** â€” Understand the user's goal, ask clarifying questions if ambiguous
2. **Research** â€” Use Google Search grounding to gather real-world context
3. **Architect** â€” Design the project structure (Phases â†’ Tasks â†’ Subtasks â†’ Dependencies)
4. **Estimate** â€” Calculate realistic durations using bottom-up estimation with complexity-based buffers
5. **Validate** â€” Review the plan through automated quality control loops (max 2 iterations)
6. **Refine** â€” Allow the user to chat with a Project Manager agent to adjust the plan

The name "Kanso" (ç°¡ç´ ) comes from the Japanese aesthetic principle meaning **simplicity and elimination of clutter** â€” turning complex project planning into a simple, beautiful experience.

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ¤– **Multi-Agent Pipeline** | 7 specialized AI agents (Analyst, Researcher, Architect, Estimator, 3 Reviewers) collaborate to create plans |
| ğŸ”„ **Validation Loops** | Structure and estimate reviewers can reject and request revisions (up to 2 iterations) |
| ğŸ“Š **Interactive Gantt Chart** | Visualize timeline with duration bars, buffer segments (diagonal stripes), and phase grouping |
| ğŸ’¬ **Chat Refinement** | Conversational Project Manager agent modifies the plan â€” "make it shorter", "add a testing phase", "remove buffers" |
| ğŸ“ **File Upload** | Attach reference documents (PDF, images, text) for additional context during analysis |
| âš¡ **Real-time Agent Status** | WebSocket connection shows which agent is active, with iteration badges during validation loops |
| ğŸ“… **Calendar Export** | Export your plan to Google Calendar or Outlook (.ics format) |
| ğŸ” **Google Search Grounding** | Agents use Google Search via ADK tools to research best practices and validate technical terms |
| ğŸ“ˆ **Opik Observability** | Full LLM tracing, LLM-as-judge evaluation, cost/token tracking via Comet Opik (optional) |

---

## ğŸ¤– How It Works â€” The Agent Pipeline

### Overview

When a user submits a project idea, the backend orchestrates a **sequential agent pipeline** where each agent builds on the previous one's output. The pipeline includes two **validation loops** with a maximum of 2 retry iterations each.

```
USER INPUT: "Plan a 2-week Japan trip for 2 people, $5000 budget"
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. ANALYST AGENT  (Gemini 2.5 Pro)                      â”‚
â”‚     â€¢ Checks if the request is clear and complete        â”‚
â”‚     â€¢ Uses Google Search to validate terms/URLs          â”‚
â”‚     â€¢ Returns clarifying questions OR signals "ready"    â”‚
â”‚     Output: ClarificationOutput {needsClarification,     â”‚
â”‚             questions[], reasoning}                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”‚  If questions â†’ sent to user â†’ user answers â†’ re-analyze
    â”‚  If ready â†’ proceed
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. ARCHITECT AGENT  (Gemini 2.5 Pro)                    â”‚
â”‚     â€¢ Researches domain best practices via Google Search â”‚
â”‚     â€¢ Creates: Phases â†’ Tasks â†’ Subtasks â†’ Dependencies â”‚
â”‚     â€¢ Each task gets: id, name, phase, complexity,       â”‚
â”‚       description, subtasks[], dependencies[]            â”‚
â”‚     Output: ProjectPlanOutput {projectTitle, tasks[]}    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. STRUCTURE REVIEWER  (Gemini 2.5 Flash)               â”‚
â”‚     â€¢ Validates dependency logic (can't test before build)â”‚
â”‚     â€¢ Checks completeness (software project needs testing)â”‚
â”‚     â€¢ Verifies subtask specificity (no vague "do stuff") â”‚
â”‚     â€¢ Ensures dependency IDs reference real task IDs     â”‚
â”‚     Output: ValidationOutput {isValid, critique}         â”‚
â”‚                                                          â”‚
â”‚     â”Œâ”€â”€â”€ If INVALID â”€â”€â–º Architect retries with critique  â”‚
â”‚     â”‚    (up to 2 iterations total)                      â”‚
â”‚     â””â”€â”€â”€ If VALID â”€â”€â–º proceed                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. ESTIMATOR AGENT  (Gemini 2.5 Pro)                    â”‚
â”‚     â€¢ Bottom-up estimation: estimate each subtask first  â”‚
â”‚     â€¢ Aggregates subtask durations to parent task        â”‚
â”‚     â€¢ Adds complexity-based buffers:                     â”‚
â”‚       Low=10-15%, Medium=20-25%, High=25-30%             â”‚
â”‚     â€¢ Sets duration and buffer fields (in hours)         â”‚
â”‚     Output: ProjectPlanOutput (with durations & buffers) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. ESTIMATE REVIEWER  (Gemini 2.5 Flash)                â”‚
â”‚     â€¢ Sanity-checks: "1 hour to build entire backend"=NO â”‚
â”‚     â€¢ Validates buffer inclusion (high complexity needs   â”‚
â”‚       25-30% buffer)                                     â”‚
â”‚     â€¢ Checks subtask aggregation matches parent duration â”‚
â”‚     Output: ValidationOutput {isValid, critique}         â”‚
â”‚                                                          â”‚
â”‚     â”Œâ”€â”€â”€ If INVALID â”€â”€â–º Estimator retries with critique  â”‚
â”‚     â”‚    (up to 2 iterations total)                      â”‚
â”‚     â””â”€â”€â”€ If VALID â”€â”€â–º proceed                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. FINAL REVIEWER  (Gemini 2.5 Flash)                   â”‚
â”‚     â€¢ Ensures all durations > 0                          â”‚
â”‚     â€¢ Verifies all dependency IDs exist                  â”‚
â”‚     â€¢ Cleans up formatting issues                        â”‚
â”‚     â€¢ Does NOT add or remove tasks                       â”‚
â”‚     Output: ProjectPlanOutput (cleaned)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  7. SCHEDULER (deterministic algorithm, not an LLM)      â”‚
â”‚     â€¢ Topological sort with cycle detection              â”‚
â”‚     â€¢ Calculates startOffset for each task based on      â”‚
â”‚       dependency chain (predecessor end times)           â”‚
â”‚     â€¢ Resolves parallel vs sequential execution          â”‚
â”‚     â€¢ Computes total project duration                    â”‚
â”‚     Output: Task[] with calculated startOffset values    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
  GANTT CHART rendered in browser
    â”‚
    â–¼ (user can now chat)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MANAGER AGENT  (Gemini 2.5 Pro) â€” Chat Phase            â”‚
â”‚     â€¢ "Make it shorter" â†’ reduces durations              â”‚
â”‚     â€¢ "Add a marketing phase" â†’ adds new tasks           â”‚
â”‚     â€¢ "What's the timeline?" â†’ explains without changes  â”‚
â”‚     â€¢ Must return ALL tasks (not just modified ones)      â”‚
â”‚     â€¢ Strict scope: refuses off-topic questions           â”‚
â”‚     Output: ChatOutput {reply, updatedPlan?}             â”‚
â”‚                                                          â”‚
â”‚     When updatedPlan is returned:                        â”‚
â”‚       â†’ Smart merge preserves existing task data         â”‚
â”‚       â†’ Scheduler recalculates all startOffsets          â”‚
â”‚       â†’ Gantt chart re-renders                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Validation Loop Detail

The validation loops prevent low-quality plans from reaching the user:

```
Architect creates plan
    â”‚
    â””â”€â”€â–º Structure Reviewer validates
              â”‚
              â”œâ”€â”€ PASS â†’ move to Estimator
              â”‚
              â””â”€â”€ FAIL â†’ critique sent back to Architect
                          Architect creates new plan WITH critique in prompt
                          â”‚
                          â””â”€â”€â–º Structure Reviewer validates again
                                    â”‚
                                    â”œâ”€â”€ PASS â†’ move to Estimator
                                    â””â”€â”€ FAIL â†’ accept as-is (max 2 iterations reached)
```

The same pattern applies between the Estimator and Estimate Reviewer.

**Why max 2 iterations?** Empirically, more iterations lead to diminishing returns and increased latency. Two passes catch most structural and estimation issues.

### Smart Task Merging (Chat Phase)

When the Manager agent returns an `updatedPlan`, the orchestrator runs a **merge algorithm** because LLMs sometimes return incomplete data:

1. **Raw value detection** â€” Check the raw LLM response for each field BEFORE Pydantic validation (important because Pydantic coerces `null` â†’ default values)
2. **Selective update** â€” Only update `duration`/`buffer` if the LLM explicitly provided non-null values; otherwise preserve the existing task's values
3. **New task detection** â€” Tasks with IDs not in the original plan are added
4. **Re-scheduling** â€” After merge, the scheduler recalculates all `startOffset` values

This prevents a common failure mode where the LLM returns `"duration": null` and the Gantt chart breaks.

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           FRONTEND                  â”‚         â”‚           BACKEND                â”‚
â”‚  React 19 + TypeScript + Vite       â”‚         â”‚  FastAPI + Python 3.11+          â”‚
â”‚  TailwindCSS for styling            â”‚         â”‚  Google ADK for agents           â”‚
â”‚                                     â”‚         â”‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   REST  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ App.tsx                     â”‚    â”‚ â—„â”€â”€â”€â”€â”€â–º â”‚  â”‚ main.py (FastAPI app)      â”‚  â”‚
â”‚  â”‚ â€¢ State: IDLE â†’ CLARIFYING  â”‚    â”‚         â”‚  â”‚ â€¢ REST endpoints           â”‚  â”‚
â”‚  â”‚   â†’ GENERATING â†’ READY      â”‚    â”‚   WS    â”‚  â”‚ â€¢ WebSocket /ws/{clientId} â”‚  â”‚
â”‚  â”‚ â€¢ Chat panel                â”‚    â”‚ â—„â•â•â•â•â•â–º â”‚  â”‚ â€¢ ConnectionManager        â”‚  â”‚
â”‚  â”‚ â€¢ File upload               â”‚    â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚         â”‚           â”‚                      â”‚
â”‚         â”‚                           â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚         â”‚  â”‚ orchestrator.py            â”‚  â”‚
â”‚  â”‚ apiService.ts               â”‚    â”‚         â”‚  â”‚ â€¢ Runs agent pipeline      â”‚  â”‚
â”‚  â”‚ â€¢ WebSocketService class    â”‚    â”‚         â”‚  â”‚ â€¢ Validation loops         â”‚  â”‚
â”‚  â”‚ â€¢ REST client functions     â”‚    â”‚         â”‚  â”‚ â€¢ Smart task merging       â”‚  â”‚
â”‚  â”‚ â€¢ Auto-reconnection logic   â”‚    â”‚         â”‚  â”‚ â€¢ Status broadcasting      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚         â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                     â”‚         â”‚           â”‚                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚         â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Components:                 â”‚    â”‚         â”‚  â”‚ Agents (Google ADK):       â”‚  â”‚
â”‚  â”‚ â€¢ AgentStatusDisplay        â”‚    â”‚         â”‚  â”‚ â€¢ analyst.py    (Pro)      â”‚  â”‚
â”‚  â”‚ â€¢ GanttChart                â”‚    â”‚         â”‚  â”‚ â€¢ architect.py  (Pro)      â”‚  â”‚
â”‚  â”‚ â€¢ ProjectDetails            â”‚    â”‚         â”‚  â”‚ â€¢ estimator.py  (Pro)      â”‚  â”‚
â”‚  â”‚ â€¢ ImpactBackground          â”‚    â”‚         â”‚  â”‚ â€¢ reviewer.py   (Flash)    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚         â”‚  â”‚ â€¢ manager.py    (Pro)      â”‚  â”‚
â”‚                                     â”‚         â”‚  â”‚ â€¢ research.py   (Flash)    â”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  â”‚ â€¢ scheduler.py  (algo)     â”‚  â”‚
                                                â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                                â”‚                                  â”‚
                                                â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                                                â”‚  â”‚ Support modules:           â”‚  â”‚
                                                â”‚  â”‚ â€¢ models.py (Pydantic)     â”‚  â”‚
                                                â”‚  â”‚ â€¢ output_schemas.py        â”‚  â”‚
                                                â”‚  â”‚ â€¢ config.py (env settings) â”‚  â”‚
                                                â”‚  â”‚ â€¢ opik_service.py (traces) â”‚  â”‚
                                                â”‚  â”‚ â€¢ logging_config.py        â”‚  â”‚
                                                â”‚  â”‚ â€¢ middleware.py (CORS,logs) â”‚  â”‚
                                                â”‚  â”‚ â€¢ calendar_export.py       â”‚  â”‚
                                                â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Communication Flow

1. **User types a project idea** in the frontend
2. **Frontend opens a WebSocket** connection to `/ws/{clientId}`
3. **Frontend sends** `{"action": "analyze", "topic": "..."}` over WebSocket
4. **Backend orchestrator** runs the agent pipeline, sending `AgentStatusUpdate` messages over WebSocket at each step
5. **Frontend renders** the active agent in `AgentStatusDisplay` (with iteration badges during validation loops)
6. **When complete**, backend sends the final `ProjectData` over WebSocket
7. **Frontend renders** the Gantt chart and enables the chat panel
8. **User chats** â†’ messages go via WebSocket `{"action": "chat", ...}` â†’ Manager agent responds â†’ Gantt updates

---

## ğŸ—‚ï¸ Project Structure

```
kanso-ai/
â”œâ”€â”€ README.md                           # This file
â”‚
â”œâ”€â”€ frontend/                           # React + TypeScript + Vite
â”‚   â”œâ”€â”€ index.html                      # HTML entry point
â”‚   â”œâ”€â”€ index.tsx                       # React DOM root
â”‚   â”œâ”€â”€ App.tsx                         # Main component (state machine: IDLEâ†’CLARIFYINGâ†’GENERATINGâ†’READY)
â”‚   â”œâ”€â”€ types.ts                        # TypeScript interfaces (Task, ProjectData, AgentStatus, etc.)
â”‚   â”œâ”€â”€ vite.config.ts                  # Vite build config with env variable injection
â”‚   â”œâ”€â”€ tsconfig.json                   # TypeScript compiler config
â”‚   â”œâ”€â”€ package.json                    # Dependencies: react 19, vite 6, typescript 5.8
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ AgentStatusDisplay.tsx      # Shows active agent with progress steps & iteration badges
â”‚   â”‚   â”œâ”€â”€ GanttChart.tsx              # Interactive Gantt chart (duration bars + buffer stripes)
â”‚   â”‚   â”œâ”€â”€ ProjectDetails.tsx          # Expandable task tree with subtasks and assumptions
â”‚   â”‚   â””â”€â”€ ImpactBackground.tsx        # Animated gradient background
â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ apiService.ts              # REST client + WebSocketService class with auto-reconnect
â”‚
â”œâ”€â”€ backend/                            # FastAPI + Google ADK
â”‚   â”œâ”€â”€ run.py                          # Entry point: starts uvicorn server
â”‚   â”œâ”€â”€ pyproject.toml                  # Python project config & dependencies (uv/pip compatible)
â”‚   â”œâ”€â”€ .env.example                    # Template for environment variables
â”‚   â”œâ”€â”€ README.md                       # Backend-specific docs (Opik setup, API details)
â”‚   â”œâ”€â”€ run_evaluation.py               # CLI: seed datasets & run Opik experiments
â”‚   â”œâ”€â”€ optimize_prompts.py             # Opik Agent Optimizer (MetaPromptOptimizer)
â”‚   â”œâ”€â”€ setup_online_rules.py           # Sets up 4 automated Opik online eval rules via REST API
â”‚   â””â”€â”€ app/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py                   # Pydantic Settings â€” loads .env, defines all config
â”‚       â”œâ”€â”€ models.py                   # Pydantic models for API request/response (Task, ProjectData, etc.)
â”‚       â”œâ”€â”€ main.py                     # FastAPI app: REST endpoints + WebSocket + ConnectionManager
â”‚       â”œâ”€â”€ middleware.py               # CORS middleware, request logging, correlation IDs
â”‚       â”œâ”€â”€ logging_config.py           # Structured logging: JSON (prod) / human-readable (dev), rotation
â”‚       â”œâ”€â”€ calendar_export.py          # .ics calendar file generation for Google Calendar / Outlook
â”‚       â””â”€â”€ agents/
â”‚           â”œâ”€â”€ __init__.py             # Exports all agent factories and constants
â”‚           â”œâ”€â”€ orchestrator.py         # Pipeline coordinator: runs agents, validation loops, merging
â”‚           â”œâ”€â”€ analyst.py              # Analyst agent factory â€” request analysis & clarification
â”‚           â”œâ”€â”€ architect.py            # Architect agent factory â€” project structure design
â”‚           â”œâ”€â”€ estimator.py            # Estimator agent factory â€” bottom-up time estimation
â”‚           â”œâ”€â”€ reviewer.py             # 3 reviewer factories + MAX_VALIDATION_ITERATIONS=2
â”‚           â”œâ”€â”€ manager.py              # Manager agent factory â€” chat-based plan refinement
â”‚           â”œâ”€â”€ research.py             # Research agent â€” Google Search grounding & URL content extraction
â”‚           â”œâ”€â”€ scheduler.py            # Deterministic scheduler â€” topological sort for startOffset
â”‚           â”œâ”€â”€ output_schemas.py       # Pydantic output schemas for agent responses (with field_validators)
â”‚           â”œâ”€â”€ tools.py                # Shared tools: get_current_date(), google_search
â”‚           â”œâ”€â”€ opik_service.py         # Opik integration: tracing, LLM-as-judge eval, cost tracking
â”‚           â””â”€â”€ evaluation.py           # Evaluation framework: benchmark dataset, 9 metrics, experiment runners
â”‚
â”œâ”€â”€ OPIK_INTEGRATION.md                 # Deep-dive: Opik integration docs (datasets, metrics, optimizer, rules)
â””â”€â”€ .gitignore                          # Ignores: .env, node_modules, __pycache__, .venv, venv, dist
```

### File-by-File Guide

#### Backend Core

| File | Lines | Purpose | Key Concepts |
|------|-------|---------|-------------|
| `config.py` | ~70 | Environment configuration | Pydantic `BaseSettings`, `@lru_cache` singleton, loads `.env` |
| `models.py` | ~166 | API data models | `Task`, `Subtask`, `ProjectData`, `ChatMessage`, `AgentStatusUpdate` â€” mirrors frontend `types.ts` |
| `main.py` | ~585 | FastAPI application | REST endpoints (`/api/analyze`, `/api/generate`, `/api/chat`), WebSocket handler, `ConnectionManager` for multi-client support |
| `middleware.py` | ~172 | HTTP middleware | CORS config, request logging with correlation IDs, timing |
| `logging_config.py` | ~333 | Logging system | JSON formatter (production), console formatter (dev), log rotation, `@log_execution_time` decorator |
| `calendar_export.py` | â€” | Calendar export | Converts `ProjectData` â†’ `.ics` file with configurable hours/day and weekend settings |

#### Backend Agents

| File | Lines | Model Used | Purpose | Input â†’ Output |
|------|-------|-----------|---------|---------------|
| `analyst.py` | ~80 | Gemini 2.5 Pro | Analyzes requests, asks clarifying questions | Topic string â†’ `ClarificationOutput` |
| `architect.py` | ~73 | Gemini 2.5 Pro | Designs project structure with phases & dependencies | Topic + context â†’ `ProjectPlanOutput` |
| `estimator.py` | ~71 | Gemini 2.5 Pro | Bottom-up time estimation with buffers | `ProjectPlanOutput` â†’ `ProjectPlanOutput` (with durations) |
| `reviewer.py` | ~129 | Gemini 2.5 Flash | 3 reviewers: structure, estimate, final | Plan â†’ `ValidationOutput` {isValid, critique} |
| `manager.py` | ~120 | Gemini 2.5 Pro | Chat-based plan refinement | Plan + user message â†’ `ChatOutput` {reply, updatedPlan?} |
| `research.py` | ~486 | Gemini 2.5 Flash | Google Search grounding, URL content extraction | Query â†’ search results / URL content |
| `orchestrator.py` | ~813 | (coordinator) | Runs the full pipeline, validation loops, task merging | Topic â†’ `ProjectData` (via status callbacks) |
| `scheduler.py` | ~83 | (algorithm) | Topological sort for dependency-aware scheduling | `Task[]` â†’ `Task[]` with `startOffset` |
| `output_schemas.py` | ~90 | â€” | Pydantic schemas for agent outputs | `field_validators` coerce `None` â†’ defaults |
| `tools.py` | ~25 | â€” | Shared agent tools | `get_current_date()`, `google_search` |
| `opik_service.py` | ~790 | â€” | Observability integration | LLM tracing, LLM-as-judge evaluation, cost tracking |

#### Frontend

| File | Lines | Purpose | Key Concepts |
|------|-------|---------|-------------|
| `App.tsx` | ~598 | Main component | State machine (`IDLE`â†’`CLARIFYING`â†’`GENERATING`â†’`READY`), chat panel, file upload, error handling |
| `types.ts` | ~65 | Type definitions | `Task`, `ProjectData`, `AgentType` enum, `AppState`, `ViewMode` |
| `apiService.ts` | ~464 | API client | `WebSocketService` class (connect, reconnect, message handling), REST functions, `recalculateSchedule()` |
| `AgentStatusDisplay.tsx` | â€” | Agent progress UI | Compact numbered pipeline (1â†’2â†’3â†’4), single active agent card with glow, iteration badges |
| `GanttChart.tsx` | ~441 | Gantt chart | SVG-based bars with duration + buffer (diagonal stripe pattern), phase grouping, tooltips, calendar export |
| `ProjectDetails.tsx` | â€” | Task details | Expandable task tree, subtask list, assumptions display |
| `ImpactBackground.tsx` | â€” | Visual effect | Animated gradient background canvas |

---

## ğŸš€ Getting Started

### Prerequisites

| Requirement | Version | Purpose |
|------------|---------|---------|
| **Python** | 3.11+ | Backend runtime |
| **Node.js** | 18+ | Frontend build/dev |
| **uv** (recommended) or **pip** | latest | Python package manager |
| **Google AI API Key** | â€” | Required â€” powers all Gemini agents |

Get a Google AI API key from [Google AI Studio](https://aistudio.google.com/apikey) (free tier available).

### 1. Clone the Repository

```bash
git clone https://github.com/Praveen2795/kanso-ai.git
cd kanso-ai
```

### 2. Set Up the Backend

```bash
cd backend

# Option A: Using uv (recommended â€” faster, auto-creates venv)
uv sync

# Option B: Using pip
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -e .
```

Configure environment:

```bash
cp .env.example .env
```

Edit `backend/.env` and add your Google AI API key:

```env
# Required
GOOGLE_API_KEY=your_google_api_key_here

# Optional: Opik observability (see Opik section below)
# OPIK_API_KEY=your_opik_api_key_here
# OPIK_WORKSPACE=your_workspace_name
```

Start the backend server:

```bash
# Using uv
uv run python run.py

# Using pip/venv
python run.py
```

The API will be available at `http://localhost:8000`. Visit `http://localhost:8000/docs` for interactive Swagger UI.

### 3. Set Up the Frontend

```bash
cd frontend

# Install dependencies
npm install

# Start development server
npm run dev
```

The app will be available at `http://localhost:5173`.

### 4. Use It

1. Open `http://localhost:5173` in your browser
2. Type a project idea (e.g., "Plan a 2-week Japan trip for 2 people")
3. Answer any clarifying questions the Analyst asks
4. Watch the agents build your plan in real-time
5. View the Gantt chart and chat with the Manager to refine it

---

## âš™ï¸ Configuration Reference

### Backend Environment Variables (`backend/.env`)

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `GOOGLE_API_KEY` | **Yes** | â€” | Google AI API key for Gemini models |
| `HOST` | No | `0.0.0.0` | Server bind host |
| `PORT` | No | `8000` | Server port |
| `CORS_ORIGINS` | No | `http://localhost:5173,http://localhost:3000` | Comma-separated allowed CORS origins |
| `DEFAULT_MODEL` | No | `gemini-2.5-flash` | Model for reviewers & research (fast, cheaper) |
| `PRO_MODEL` | No | `gemini-2.5-pro` | Model for analyst, architect, estimator, manager (higher quality) |
| `ENVIRONMENT` | No | `development` | `development` / `staging` / `production` â€” affects log format |
| `LOG_LEVEL` | No | `INFO` | `DEBUG` / `INFO` / `WARNING` / `ERROR` / `CRITICAL` |
| `ENABLE_FILE_LOGGING` | No | `false` | Write logs to `./logs/` with rotation |
| `LOG_MAX_BYTES` | No | `10485760` (10MB) | Max log file size before rotation |
| `LOG_BACKUP_COUNT` | No | `5` | Number of rotated log files to keep |
| `OPIK_API_KEY` | No | â€” | Comet Opik API key for observability |
| `OPIK_WORKSPACE` | No | â€” | Comet Opik workspace name |
| `OPIK_PROJECT_NAME` | No | `kanso-ai` | Opik project name |

### Frontend Environment Variables (`frontend/.env`)

| Variable | Default | Description |
|----------|---------|-------------|
| `VITE_API_URL` | `http://localhost:8000` | Backend REST API URL |
| `VITE_WS_URL` | `ws://localhost:8000` | Backend WebSocket URL |

---

## ğŸ“¡ API Reference

### REST Endpoints

| Endpoint | Method | Description | Request Body | Response |
|----------|--------|-------------|-------------|----------|
| `/health` | GET | Health check | â€” | `{"status": "healthy"}` |
| `/api/analyze` | POST | Analyze a project request | `{topic, chatHistory[]}` | `{needsClarification, questions[], reasoning}` |
| `/api/generate` | POST | Generate complete plan | `{topic, context, file?}` | `ProjectData` |
| `/api/chat` | POST | Chat with project manager | `{project, message, history[]}` | `{reply, updatedPlan?}` |
| `/api/export-calendar` | POST | Export plan as .ics | `{project, startDate?, hoursPerDay?, includeWeekends?}` | `.ics` file |
| `/api/recalculate` | POST | Recalculate task scheduling | `{tasks[]}` | `{tasks[], totalDuration}` |
| `/api/opik/status` | GET | Opik observability status | â€” | `{enabled, workspace, traces_url}` |

### WebSocket Protocol

**Endpoint:** `ws://localhost:8000/ws/{clientId}`

**Client â†’ Server Messages:**

```jsonc
// Analyze a project request
{"action": "analyze", "topic": "Build a mobile app", "chatHistory": []}

// Generate a full plan (after analysis)
{"action": "generate", "topic": "Build a mobile app", "context": "Additional context...", "file": null}

// Chat with manager (after plan is generated)
{"action": "chat", "project": { /* ProjectData */ }, "message": "Make it shorter", "history": []}

// Keep-alive ping
{"action": "ping"}
```

**Server â†’ Client Messages:**

```jsonc
// Agent status update (during generation)
{"type": "agent_status", "data": {"active": true, "agent": "Architect", "message": "Designing project structure..."}}

// Analysis result
{"type": "analysis", "data": {"needsClarification": true, "questions": ["What is your budget?"], "reasoning": "..."}}

// Generated plan
{"type": "plan", "data": { /* ProjectData */ }}

// Chat response
{"type": "chat_response", "data": {"reply": "I've shortened the timeline.", "updatedPlan": { /* ProjectData */ }}}

// Error
{"type": "error", "data": {"message": "Something went wrong"}}

// Pong response
{"type": "pong"}
```

### Example: Generate a Plan via cURL

```bash
curl -X POST http://localhost:8000/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "topic": "Launch a Shopify store in 30 days",
    "context": "Budget is $5000, selling handmade jewelry, solo founder"
  }'
```

---

## ğŸ“¦ Data Models

### Core Types (shared between frontend & backend)

```
Task
â”œâ”€â”€ id: string              # Unique identifier (e.g., "phase1_task2")
â”œâ”€â”€ name: string            # Display name
â”œâ”€â”€ phase: string           # Phase grouping (e.g., "Phase 1: Research")
â”œâ”€â”€ startOffset: number     # Hours from project start (calculated by scheduler)
â”œâ”€â”€ duration: number        # Estimated hours of work
â”œâ”€â”€ buffer: number          # Buffer hours (complexity-based, 10-30% of duration)
â”œâ”€â”€ dependencies: string[]  # IDs of tasks that must complete first
â”œâ”€â”€ description?: string    # Detailed description
â”œâ”€â”€ complexity: "Low" | "Medium" | "High"
â””â”€â”€ subtasks: Subtask[]
    â”œâ”€â”€ name: string
    â”œâ”€â”€ description?: string
    â””â”€â”€ duration: number    # Hours

ProjectData
â”œâ”€â”€ title: string
â”œâ”€â”€ description: string
â”œâ”€â”€ assumptions: string[]   # AI-generated planning assumptions
â”œâ”€â”€ tasks: Task[]
â””â”€â”€ totalDuration: number   # Total project hours (max task endTime)
```

### Agent Output Schemas (`output_schemas.py`)

These Pydantic models define the **structured output** that each agent must return. Google ADK enforces these schemas via the `output_schema` parameter on `LlmAgent`.

Key `field_validators` (prevent LLM null values from breaking the pipeline):
- `TaskOutput.duration`: `None` â†’ `1.0` (default 1 hour)
- `TaskOutput.buffer`: `None` â†’ `0.0`
- `SubtaskOutput.duration`: `None` â†’ `0.5` (default 30 min)

---

## ğŸ”­ Observability & Evaluation with Opik

Kanso.AI deeply integrates **[Opik](https://github.com/comet-ml/opik)** by Comet across the entire pipeline â€” from real-time trace collection during agent execution, through offline evaluation experiments with curated datasets, to automated prompt optimization and continuous production monitoring.

> ğŸ“„ **Even more details**: [OPIK_INTEGRATION.md](OPIK_INTEGRATION.md) â€” full architecture diagrams, metric scoring logic, hackathon alignment matrix, and all CLI commands.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Opik Integration in Kanso.AI                            â”‚
â”‚                                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚  1. Tracing   â”‚    â”‚ 2. Datasets  â”‚    â”‚  3. Online   â”‚    â”‚ 4. Agent     â”‚  â”‚
â”‚   â”‚   OpikTracer  â”‚    â”‚  & Experimentsâ”‚    â”‚   Eval Rules â”‚    â”‚  Optimizer   â”‚  â”‚
â”‚   â”‚   (per agent) â”‚    â”‚  (9 metrics)  â”‚    â”‚  (4 rules)   â”‚    â”‚ (MetaPrompt) â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚          â”‚                   â”‚                   â”‚                   â”‚           â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                      â”‚                                           â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚
â”‚                              â”‚  Comet / Opik  â”‚                                  â”‚
â”‚                              â”‚   Dashboard    â”‚                                  â”‚
â”‚                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 1. Full-Pipeline Tracing via `OpikTracer`

Every request flows through the 7-agent pipeline with **complete trace visibility**. We use the official `OpikTracer` from Opik's ADK integration â€” every LLM call, tool invocation, agent handoff, and validation loop iteration is captured automatically:

```python
# backend/app/opik_service.py
from opik.integrations.adk import OpikTracer, track_adk_agent_recursive

tracer = OpikTracer(
    name="kanso-pipeline",
    tags=["kanso-ai", "project-planning", "multi-agent"],
    metadata={
        "pro_model": "gemini-2.5-pro",
        "judge_model": "gemini/gemini-2.5-flash",
        "framework": "google-adk",
    },
    project_name="kanso-ai",
)

# Recursively instrument ALL sub-agents so every LLM call is traced
instrumented_agent = track_adk_agent_recursive(agent, tracer)
```

**What gets captured per trace:**
- Full LLM input/output payloads for every agent (Analyst â†’ Researcher â†’ Architect â†’ Reviewers â†’ Estimator â†’ Manager)
- Token usage (input + output tokens) per agent call
- Tool calls (Google Search grounding, URL extraction)
- Validation loop iterations and reviewer accept/reject decisions
- End-to-end pipeline timing with millisecond precision

The tracing is **non-intrusive** â€” if `OPIK_API_KEY` is not set, all Opik code silently degrades to no-ops via try/except guards:

```python
try:
    import opik
    from opik import track, opik_context
    from opik.integrations.adk import OpikTracer
    _opik_available = True
except ImportError:
    # Create dummy decorators â€” app works without Opik
    def track(*args, **kwargs): ...
```

---

### 2. Rich Trace Metadata

Beyond basic LLM input/output, we **enrich every trace** with structured metadata at three levels using `opik_context.update_current_span()` and `opik_context.update_current_trace()`:

#### Pipeline-Level Metadata (on the root trace)

Attached at the end of `generate_project_plan()` â€” gives a complete picture of the entire pipeline run:

```python
# backend/app/agents/orchestrator.py â€” generate_project_plan()
opik_context.update_current_trace(
    metadata={
        "pipeline_elapsed_seconds": total_elapsed,      # Total wall-clock time
        "stage_timings": {                               # Per-stage breakdown
            "architecture": 12.5,                        #   Architect + Structure Reviewer
            "estimation": 8.3,                           #   Estimator + Estimate Reviewer
            "finalize": 2.1,                             #   Final Reviewer + Scheduling
        },
        "architecture_iterations": 1,                    # How many architect retries
        "estimation_iterations": 2,                      # How many estimator retries
        "structure_validated": True,                      # Did structure pass review?
        "estimates_validated": True,                      # Did estimates pass review?
        "has_research_context": True,                     # Was Google Search used?
        "complexity_distribution": {                      # Task complexity breakdown
            "Low": 3, "Medium": 5, "High": 2
        },
        "task_count": 10,
        "total_duration_hours": 42.5,
        "phases": ["Design", "Development", "Testing"],
    }
)
```

#### Agent-Level Metadata (per agent span)

Each agent execution in `run_agent_with_status()` records its own span metadata:

```python
# backend/app/agents/orchestrator.py â€” run_agent_with_status()
opik_context.update_current_span(
    metadata={
        "agent_name": "architect",
        "agent_type": "pro_model",           # Which model tier (pro vs default)
        "execution_time_ms": 4523.7,         # Agent wall-clock time
        "response_length": 12840,            # Character count of agent output
        "model": "gemini-2.5-pro",
    }
)
```

#### Function-Level Metadata (specific orchestrator functions)

Individual functions like `analyze_request()` and `chat_with_manager()` record domain-specific metrics:

| Function | Tracked Fields |
|----------|---------------|
| `analyze_request()` | `topic_length`, `has_chat_history`, `needs_clarification`, `question_count`, `elapsed_seconds` |
| `chat_with_manager()` | `message_length`, `history_turns`, `project_task_count`, `has_plan_update`, `reply_length`, `elapsed_seconds` |
| `generate_project_plan()` | `pipeline`, `agents`, `pro_model`, `default_model`, `max_validation_iterations` |

---

### 3. Datasets & Experiments

We created a curated **benchmark dataset** (`kanso-planning-benchmark`) with **12 diverse project planning requests** stored in Opik, covering different complexity levels, domains, and edge cases:

| Category | Count | Examples | Purpose |
|----------|-------|----------|---------|
| **Simple/Clear** | 2 | Portfolio website, TODO API | Baseline â€” agent should NOT over-complicate |
| **Medium Complexity** | 3 | E-commerce platform, fitness app, CI/CD pipeline | Should handle multi-phase planning |
| **Complex/Enterprise** | 2 | Real-time whiteboard, fintech API gateway | Many dependencies, compliance, security |
| **Vague/Ambiguous** | 2 | "I want to build an app", "Make something with AI" | Tests analyst's clarification detection |
| **Hackathon-Themed** | 2 | Habit tracker, AI daily planner | Aligned with "Productivity & Work Habits" track |
| **Edge Case/Specific** | 1 | Distributed task queue with Redis Streams | Highly technical, tests domain understanding |

Each dataset item includes rich metadata for metric evaluation:

```python
{
    "input": "Build a real-time collaborative whiteboard with video chat",
    "context": "EdTech startup. WebSocket + WebRTC. Team of 4, 3 months.",
    "expected_traits": {
        "min_tasks": 10, "max_tasks": 25,
        "expected_phases": ["Architecture", "Core Canvas", "Video Integration", ...],
        "complexity": "complex",
        "domain": "fullstack"
    },
    "difficulty": "hard",
    "tags": ["fullstack", "real-time", "webrtc", "complex"]
}
```

**Two experiment types** run against this dataset:

| Experiment | Pipeline Tested | Agents Involved | Metrics Applied |
|------------|----------------|-----------------|-----------------|
| `plan` (full pipeline) | All 6 agents end-to-end | Researcher â†’ Architect â†’ Reviewers â†’ Estimator â†’ Manager | 9 metrics |
| `analyst` (analyst only) | Analyst agent only | Analyst | 4 metrics |

```bash
# Seed dataset (idempotent)
uv run python run_evaluation.py --seed

# Run named experiments
uv run python run_evaluation.py --experiment plan --name "plan-v3"
uv run python run_evaluation.py --experiment analyst --name "analyst-v2"
```

---

### 4. Custom & Built-in Evaluation Metrics (9 Total)

Experiments combine **5 custom domain-specific metrics** we wrote as `opik.BaseMetric` subclasses with **4 Opik built-in LLM-as-judge metrics** (powered by Gemini 2.5 Flash via LiteLLM):

#### Custom Metrics (deterministic heuristics â€” zero API calls, instant scoring)

| Metric Class | Name | What It Measures | Scoring Logic |
|-------------|------|------------------|---------------|
| `PlanHasRequiredFields` | `plan_structure_completeness` | All required plan fields present | Checks 9 structural fields: title, tasks, IDs, names, phases, durations, dependencies, subtasks, summary. Score = proportion present. |
| `TaskCountReasonableness` | `task_count_reasonableness` | Task count within expected range | Compares `len(tasks)` against `expected_traits.min_tasks`/`max_tasks`. Perfect if in range, scaled down proportionally outside. |
| `DurationRealism` | `duration_realism` | Realistic time estimates | Flags tasks < 0.5h or > 80h, buffer > duration. Score = 1 - (issues / total_tasks). |
| `PlanQualityLLMJudge` | `plan_quality_llm_judge` | Holistic plan quality | Uses Gemini to score requirement coverage, task granularity, logical flow, completeness (4 dimensions Ã— 0.25 each). |
| `ClarificationQualityJudge` | `clarification_quality` | Analyst ambiguity detection | Uses Gemini to score correct detection, question quality, reasoning quality for the analyst agent. |

#### Opik Built-in LLM-as-Judge Metrics (via `gemini/gemini-2.5-flash`)

| Built-in Class | What It Measures | Configuration |
|---------------|------------------|---------------|
| `Hallucination` | Fabricated info not grounded in input | `model="gemini/gemini-2.5-flash"` |
| `AnswerRelevance` | Does the plan address the original request? | `model="gemini/gemini-2.5-flash"`, `require_context=False` |
| `Moderation` | Content safety check | `model="gemini/gemini-2.5-flash"` |
| `GEval` | Custom criteria evaluation | Task: "evaluating an AI-generated project plan", Criteria: logical tasks, realistic estimates, technology choices, request coverage, valid JSON |
| `IsJson` | Valid JSON output check | No model needed â€” deterministic |

```python
# backend/app/evaluation.py â€” run_plan_quality_experiment()
from opik.evaluation.metrics import Hallucination, AnswerRelevance, Moderation, GEval, IsJson

metrics = [
    # Custom heuristic metrics (instant, no API calls)
    TaskCountReasonableness(),
    PlanHasRequiredFields(),
    DurationRealism(),
    # Custom LLM-as-judge
    PlanQualityLLMJudge(),
    # Opik built-in LLM-as-judge (via LiteLLM â†’ Gemini)
    Hallucination(model=JUDGE_MODEL),
    AnswerRelevance(model=JUDGE_MODEL, require_context=False),
    Moderation(model=JUDGE_MODEL),
    IsJson(),
    GEval(
        model=JUDGE_MODEL,
        task_introduction="You are evaluating an AI-generated software project plan.",
        evaluation_criteria="The plan should: 1) Break the project into logical tasks. "
            "2) Have realistic time estimates. 3) Cover all aspects of the request. "
            "4) Be structured as valid JSON.",
    ),
]

result = evaluate(
    experiment_name="plan-quality-v1",
    dataset=dataset,
    task=plan_generation_task,
    scoring_metrics=metrics,
)
```

---

### 5. Online Evaluation Rules

Four **automated LLM-as-judge rules** are configured via the Opik REST API to run on **every production trace** â€” no manual experiment needed:

| Rule | Metric | What It Does | Sampling |
|------|--------|-------------|----------|
| **Hallucination Detection** | Custom LLM Judge | Flags fabricated technologies, unrealistic claims, invented capabilities | 100% |
| **Content Safety** | Content Moderation | Checks for offensive language, PII exposure, harmful instructions, bias | 100% |
| **Plan Relevance** | Answer Relevance | Verifies the plan addresses the user's stated goals and context | 100% |
| **Plan Quality** | Domain Expert Judge | Evaluates task decomposition, dependency logic, time estimates, actionability | 100% |

These rules are created programmatically via the Opik REST API:

```python
# backend/setup_online_rules.py
POST https://www.comet.com/opik/api/v1/private/automations/evaluators/
{
    "type": "llm_as_judge",
    "name": "Hallucination Detection",
    "project_ids": ["019c1bb1-..."],
    "sampling_rate": 1.0,
    "enabled": true,
    "action": "evaluator",
    "code": {
        "model": {"name": "gpt-4o-mini", "temperature": 0},
        "messages": [{
            "role": "USER",
            "content": "You are an expert judge... Score: 0.0 = no hallucination, 1.0 = severe...\n\nINPUT:\n{{input}}\n\nOUTPUT:\n{{output}}"
        }],
        "variables": {"input": "input", "output": "output"},
        "schema": [{"name": "hallucination", "type": "DOUBLE", ...}]
    }
}
```

```bash
# One-time setup (idempotent â€” skips existing rules)
uv run python setup_online_rules.py

# Dry-run to preview without creating
uv run python setup_online_rules.py --dry-run
```

---

### 6. Opik Agent Optimizer

We use the **`opik-optimizer` SDK** (v3.0.1) with `MetaPromptOptimizer` to automatically improve agent system prompts through iterative refinement:

| What's Optimized | Agent | Optimization Metric | Metric Type |
|-----------------|-------|---------------------|-------------|
| Clarification detection prompt | Analyst | JSON structure, question quality, reasoning, alignment with expected traits | Fast heuristic (0 API calls) |
| Plan generation prompt | Architect | Plan structure, task count, required fields, content relevance | Fast heuristic (0 API calls) |

**Why fast heuristic metrics?** The optimizer evaluates many candidate prompts Ã— many dataset samples per trial. LLM-as-judge metrics are too slow and expensive for this inner loop. Our deterministic heuristic metrics return scores in microseconds, enabling the optimizer to explore more of the prompt space:

```python
# backend/optimize_prompts.py
from opik_optimizer import MetaPromptOptimizer, ChatPrompt

prompt = ChatPrompt(
    messages=[
        {"role": "system", "content": ANALYST_INITIAL_PROMPT},
        {"role": "user", "content": "{question}"},
    ],
    model="gemini/gemini-2.5-pro",
)

optimizer = MetaPromptOptimizer(model="gemini/gemini-2.5-pro")

result = optimizer.optimize_prompt(
    prompt=prompt,
    dataset=dataset,              # 12 benchmark items
    metric=analyst_metric,         # Fast heuristic scorer
    max_trials=3,
    n_samples=6,
)
result.display()  # Shows baseline â†’ optimized score progression
```

```bash
# Run optimization
uv run python optimize_prompts.py --agent analyst --trials 3 --samples 6
uv run python optimize_prompts.py --agent architect --trials 3 --samples 6

# Dry-run (no API calls, just shows config)
uv run python optimize_prompts.py --agent analyst --dry-run
```

---

### 7. Experiment Results

#### Plan Quality Experiment (full 6-agent pipeline, 12 samples)

| Metric | Score | Interpretation |
|--------|-------|----------------|
| `plan_structure_completeness` | **1.0000** | All 9 structural fields present in every plan |
| `g_eval_metric` | **0.9833** | Opik's GEval rates plans as excellent |
| `answer_relevance_metric` | **0.9758** | Plans directly address original requests |
| `task_count_reasonableness` | **0.9075** | Task counts match expected ranges for project complexity |
| `duration_realism` | **0.8901** | Realistic time estimates with appropriate buffers |
| `hallucination_metric` | **0.7125** | Some introduced context (expected â€” plans add suggestions) |
| `plan_quality_llm_judge` | **0.5892** | Conservative holistic judge (strictest metric) |
| `is_json_metric` | **1.0000** | All pipeline outputs are valid JSON |
| `moderation_metric` | **0.0000** | All content is safe (lower = better) |

#### Analyst Experiment (analyst agent only, 12 samples)

| Metric | Score |
|--------|-------|
| `clarification_quality` | **0.6375** |
| `answer_relevance_metric` | **0.6875** |
| `is_json_metric` | **1.0000** |
| `moderation_metric` | **0.0000** |

#### Optimizer Baseline

| Agent | Baseline Score | Metric |
|-------|---------------|--------|
| Analyst | **0.9167** | Fast heuristic (JSON + question quality + reasoning + alignment) |

### Quick Setup

```bash
cd backend

# Required environment variables
export GOOGLE_API_KEY=your_key       # For Gemini models
export OPIK_API_KEY=your_key         # For Opik
export OPIK_WORKSPACE=your_workspace

# Seed the benchmark dataset
uv run python run_evaluation.py --seed

# Run experiments
uv run python run_evaluation.py --experiment plan --name "plan-v1"
uv run python run_evaluation.py --experiment analyst --name "analyst-v1"

# Set up online evaluation rules
uv run python setup_online_rules.py

# Run prompt optimizer
uv run python optimize_prompts.py --agent analyst --trials 3
```

Opik is **completely optional** â€” if `OPIK_API_KEY` is not set, all tracing and evaluation is silently skipped.

---

## ğŸ“š Technology Stack

### Backend

| Technology | Version | Purpose |
|------------|---------|---------|
| [Python](https://python.org/) | 3.11+ | Runtime |
| [FastAPI](https://fastapi.tiangolo.com/) | 0.115+ | Async web framework with auto-generated OpenAPI docs |
| [Google ADK](https://google.github.io/adk-docs/) | 1.23+ | Agent Development Kit â€” `LlmAgent`, `Runner`, `InMemorySessionService` |
| [Google GenAI](https://ai.google.dev/) | 1.37+ | Gemini API client â€” models, Google Search grounding |
| [Pydantic](https://docs.pydantic.dev/) | 2.10+ | Data validation, settings management, agent output schemas |
| [Uvicorn](https://www.uvicorn.org/) | 0.32+ | ASGI server |
| [Opik](https://github.com/comet-ml/opik) | 1.0+ | LLM observability, tracing, evaluation, datasets & experiments |
| [opik-optimizer](https://pypi.org/project/opik-optimizer/) | 3.0+ | Agent prompt optimization (`MetaPromptOptimizer`) |
| [uv](https://docs.astral.sh/uv/) | latest | Fast Python package manager (recommended) |

### Frontend

| Technology | Version | Purpose |
|------------|---------|---------|
| [React](https://react.dev/) | 19 | UI component library |
| [TypeScript](https://www.typescriptlang.org/) | 5.8 | Type-safe JavaScript |
| [Vite](https://vitejs.dev/) | 6 | Build tool & dev server with HMR |
| [TailwindCSS](https://tailwindcss.com/) | (CDN) | Utility-first CSS framework |

---

## ğŸ› ï¸ Development Guide

### Backend

```bash
cd backend

# Install with dev dependencies
uv sync --extra dev     # or: pip install -e ".[dev]"

# Run with hot reload
uv run uvicorn app.main:app --reload --port 8000

# Code formatting
black app/
isort app/

# Linting
flake8 app/

# Run tests
pytest tests/
```

### Frontend

```bash
cd frontend

# Dev server with hot reload
npm run dev

# Type checking
npx tsc --noEmit

# Production build
npm run build

# Preview production build
npm run preview
```

### Key Development Notes

1. **Agent factories** â€” All agents are created via factory functions (e.g., `create_architect_agent(critique=None)`). The `critique` parameter enables the retry loop â€” when a reviewer rejects, the critique is passed back to the agent factory.

2. **Model selection** â€” `PRO_MODEL` (Gemini 2.5 Pro) is used for agents that need high reasoning (analyst, architect, estimator, manager). `DEFAULT_MODEL` (Gemini 2.5 Flash) is used for reviewers and research â€” faster and cheaper.

3. **Output schemas** â€” Each agent has a Pydantic `output_schema` that Google ADK uses to force structured JSON output. The `field_validators` in `output_schemas.py` are critical safety nets for when the LLM returns null values.

4. **Scheduler is deterministic** â€” Unlike the agents, `scheduler.py` is pure Python (no LLM). It performs topological sort with cycle detection to calculate `startOffset` for each task.

5. **WebSocket is the primary channel** â€” While REST endpoints exist for `/api/analyze`, `/api/generate`, and `/api/chat`, the main flow uses WebSocket for real-time agent status updates during generation.

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Areas for Improvement

- Extract `App.tsx` into smaller components (chat panel, clarification form)
- Consolidate duplicate schemas (`output_schemas.py` and `models.py` overlap)
- Add comprehensive test coverage
- Add Docker / Docker Compose for one-command setup
- Support for additional LLM providers beyond Gemini

---

## ğŸ“„ License

This project is licensed under the Apache 2.0 License â€” see the [LICENSE](LICENSE) file for details.

---

<div align="center">

**Built with â¤ï¸ by [Praveen](https://github.com/Praveen2795)**

[Report Bug](https://github.com/Praveen2795/kanso-ai/issues) Â· [Request Feature](https://github.com/Praveen2795/kanso-ai/issues)

</div>
